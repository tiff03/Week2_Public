{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1DConvNet_Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PcahCqill_TM"
      },
      "source": [
        "# 1D Convolutional Neural Networks Tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Bx9AXfRcmGky"
      },
      "source": [
        "### About the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rkgC31U1rsVk"
      },
      "source": [
        "Data for this exercise is from the [Human Activity Recognition Using Smartphones Data Set](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones) which is furnished by the University of California Irvine Machine Learning Repository.\n",
        "<br><br>\n",
        "In order to populate the dataset, subjects wore a smartphone that recorded their linear acceleration (using an accelerometer) and angular accelleration (using a gyroscope) while performing typical daily activity. After the data was collected, segments of each data recording were labeled based on the activity of the subject during that period. (This was possible by consulting video recordings.) Possible data labels in this dataset are: WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, and LAYING. <br><br>\n",
        "\n",
        "The goal of our classifier will be to predict the label of a segment of recording based on accelerometer and gyroscope readings. \n",
        "\n",
        "(Data details: there are two sets of x, y, z accelerations in each input sample, one for the sensor itself and another for the subject's body accelerations. Thus, there are 9 different signals recorded for each labeled datapoint-- 3 linear accelerations, 3 linear \"body\" accelerations, and 3 angular accelerations. All signals have a sample rate of 50 Hz.)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z55giekhr4mQ",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "\n",
        "#array/dataframe functions\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#file manipulations\n",
        "import os\n",
        "\n",
        "#data preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#plotting functionality\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "#set default plotting fonts\n",
        "font = {'family' : 'sans-serif',\n",
        "        'weight' : 'normal',\n",
        "        'size'   : 16}\n",
        "\n",
        "matplotlib.rc('font', **font)\n",
        "\n",
        "#tensorflow and keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gHFlVzfY4ZPQ"
      },
      "source": [
        "### Read in data (using functions provided below)\n",
        "\n",
        "Before we read in test/training data, we need to load two provided functions. (read_data_test() and read_data_train()) These functions just load in the data from a github repository into test and training datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iDJBOkx9x0HZ",
        "colab": {}
      },
      "source": [
        "#Define functions to read in test/train data from files\n",
        "\n",
        "def read_data_test():\n",
        "  \"\"\" Read test data from github and store in numpy array\"\"\"\n",
        "\n",
        "  # Fixed params\n",
        "  n_class = 6\n",
        "  n_steps = 128\n",
        "\n",
        "  label_path = 'https://raw.githubusercontent.com/BeaverWorksMedlytics2020/Data_Public/master/NotebookExampleData/Week2/week2_conv1d/y_test.txt'\n",
        "  labels = pd.read_csv(label_path, header = None)\n",
        "\n",
        "  list_of_channels = ['body_acc_x', 'body_acc_y', 'body_acc_z', 'body_gyro_x',\n",
        "  'body_gyro_y', 'body_gyro_z', 'total_acc_x', 'total_acc_y', 'total_acc_z']\n",
        "\n",
        "  X = np.zeros((len(labels), n_steps, len(list_of_channels)))\n",
        "\n",
        "  label_path = 'https://raw.githubusercontent.com/BeaverWorksMedlytics2020/Data_Public/master/NotebookExampleData/Week2/week2_conv1d/IntertialSignals/body_acc_x_test.txt'\n",
        "  dat_ = pd.read_csv(label_path, delim_whitespace = True, header = None)\n",
        "  X[:,:,0] = dat_.to_numpy()\n",
        "\n",
        "  label_path = 'https://raw.githubusercontent.com/BeaverWorksMedlytics2020/Data_Public/master/NotebookExampleData/Week2/week2_conv1d/IntertialSignals/body_acc_y_test.txt'\n",
        "  dat_ = pd.read_csv(label_path, delim_whitespace = True, header = None)\n",
        "  X[:,:,1] = dat_.to_numpy()\n",
        "\n",
        "  label_path = 'https://raw.githubusercontent.com/BeaverWorksMedlytics2020/Data_Public/master/NotebookExampleData/Week2/week2_conv1d/IntertialSignals/body_acc_z_test.txt'\n",
        "  dat_ = pd.read_csv(label_path, delim_whitespace = True, header = None)\n",
        "  X[:,:,2] = dat_.to_numpy()\n",
        "\n",
        "  label_path = 'https://raw.githubusercontent.com/BeaverWorksMedlytics2020/Data_Public/master/NotebookExampleData/Week2/week2_conv1d/IntertialSignals/body_gyro_x_test.txt'\n",
        "  dat_ = pd.read_csv(label_path, delim_whitespace = True, header = None)\n",
        "  X[:,:,3] = dat_.to_numpy()\n",
        "\n",
        "  label_path = 'https://raw.githubusercontent.com/BeaverWorksMedlytics2020/Data_Public/master/NotebookExampleData/Week2/week2_conv1d/IntertialSignals/body_gyro_y_test.txt'\n",
        "  dat_ = pd.read_csv(label_path, delim_whitespace = True, header = None)\n",
        "  X[:,:,4] = dat_.to_numpy()\n",
        "\n",
        "  label_path = 'https://raw.githubusercontent.com/BeaverWorksMedlytics2020/Data_Public/master/NotebookExampleData/Week2/week2_conv1d/IntertialSignals/body_gyro_z_test.txt'\n",
        "  dat_ = pd.read_csv(label_path, delim_whitespace = True, header = None)\n",
        "  X[:,:,5] = dat_.to_numpy()\n",
        "\n",
        "  label_path = 'https://raw.githubusercontent.com/BeaverWorksMedlytics2020/Data_Public/master/NotebookExampleData/Week2/week2_conv1d/IntertialSignals/total_acc_x_test.txt'\n",
        "  dat_ = pd.read_csv(label_path, delim_whitespace = True, header = None)\n",
        "  X[:,:,6] = dat_.to_numpy()\n",
        "\n",
        "  label_path = 'https://raw.githubusercontent.com/BeaverWorksMedlytics2020/Data_Public/master/NotebookExampleData/Week2/week2_conv1d/IntertialSignals/total_acc_y_test.txt'\n",
        "  dat_ = pd.read_csv(label_path, delim_whitespace = True, header = None)\n",
        "  X[:,:,7] = dat_.to_numpy()\n",
        "\n",
        "  label_path = 'https://raw.githubusercontent.com/BeaverWorksMedlytics2020/Data_Public/master/NotebookExampleData/Week2/week2_conv1d/IntertialSignals/total_acc_z_test.txt'\n",
        "  dat_ = pd.read_csv(label_path, delim_whitespace = True, header = None)\n",
        "  X[:,:,8] = dat_.to_numpy()\n",
        "\n",
        "\n",
        "  # Return \n",
        "  return X, labels[0].values, list_of_channels\n",
        "\n",
        "\n",
        "def read_data_train():\n",
        "  \"\"\" Read train data from github and store in numpy array\"\"\"\n",
        "\n",
        "  # Fixed params\n",
        "  n_class = 6\n",
        "  n_steps = 128\n",
        "\n",
        "  label_path ='https://raw.githubusercontent.com/BeaverWorksMedlytics2020/Data_Public/master/NotebookExampleData/Week2/week2_conv1d/y_train.txt'\n",
        "  labels = pd.read_csv(label_path, header = None)\n",
        "\n",
        "  list_of_channels = ['body_acc_x', 'body_acc_y', 'body_acc_z', 'body_gyro_x',\n",
        "  'body_gyro_y', 'body_gyro_z', 'total_acc_x', 'total_acc_y', 'total_acc_z']\n",
        "\n",
        "  X = np.zeros((len(labels), n_steps, len(list_of_channels)))\n",
        "\n",
        "  label_path = 'https://raw.githubusercontent.com/BeaverWorksMedlytics2020/Data_Public/master/NotebookExampleData/Week2/week2_conv1d/IntertialSignals/body_acc_x_train.txt'\n",
        "  dat_ = pd.read_csv(label_path, delim_whitespace = True, header = None)\n",
        "  X[:,:,0] = dat_.to_numpy()\n",
        "\n",
        "  label_path = 'https://raw.githubusercontent.com/BeaverWorksMedlytics2020/Data_Public/master/NotebookExampleData/Week2/week2_conv1d/IntertialSignals/body_acc_y_train.txt'\n",
        "  dat_ = pd.read_csv(label_path, delim_whitespace = True, header = None)\n",
        "  X[:,:,1] = dat_.to_numpy()\n",
        "\n",
        "  label_path = 'https://raw.githubusercontent.com/BeaverWorksMedlytics2020/Data_Public/master/NotebookExampleData/Week2/week2_conv1d/IntertialSignals/body_acc_z_train.txt'\n",
        "  dat_ = pd.read_csv(label_path, delim_whitespace = True, header = None)\n",
        "  X[:,:,2] = dat_.to_numpy()\n",
        "\n",
        "  label_path = 'https://raw.githubusercontent.com/BeaverWorksMedlytics2020/Data_Public/master/NotebookExampleData/Week2/week2_conv1d/IntertialSignals/body_gyro_x_train.txt'\n",
        "  dat_ = pd.read_csv(label_path, delim_whitespace = True, header = None)\n",
        "  X[:,:,3] = dat_.to_numpy()\n",
        "\n",
        "  label_path = 'https://raw.githubusercontent.com/BeaverWorksMedlytics2020/Data_Public/master/NotebookExampleData/Week2/week2_conv1d/IntertialSignals/body_gyro_y_train.txt'\n",
        "  dat_ = pd.read_csv(label_path, delim_whitespace = True, header = None)\n",
        "  X[:,:,4] = dat_.to_numpy()\n",
        "\n",
        "  label_path = 'https://raw.githubusercontent.com/BeaverWorksMedlytics2020/Data_Public/master/NotebookExampleData/Week2/week2_conv1d/IntertialSignals/body_gyro_z_train.txt'\n",
        "  dat_ = pd.read_csv(label_path, delim_whitespace = True, header = None)\n",
        "  X[:,:,5] = dat_.to_numpy()\n",
        "\n",
        "  label_path = 'https://raw.githubusercontent.com/BeaverWorksMedlytics2020/Data_Public/master/NotebookExampleData/Week2/week2_conv1d/IntertialSignals/total_acc_x_train.txt'\n",
        "  dat_ = pd.read_csv(label_path, delim_whitespace = True, header = None)\n",
        "  X[:,:,6] = dat_.to_numpy()\n",
        "\n",
        "  label_path = 'https://raw.githubusercontent.com/BeaverWorksMedlytics2020/Data_Public/master/NotebookExampleData/Week2/week2_conv1d/IntertialSignals/total_acc_y_train.txt'\n",
        "  dat_ = pd.read_csv(label_path, delim_whitespace = True, header = None)\n",
        "  X[:,:,7] = dat_.to_numpy()\n",
        "\n",
        "  label_path = 'https://raw.githubusercontent.com/BeaverWorksMedlytics2020/Data_Public/master/NotebookExampleData/Week2/week2_conv1d/IntertialSignals/total_acc_z_train.txt'\n",
        "  dat_ = pd.read_csv(label_path, delim_whitespace = True, header = None)\n",
        "  X[:,:,8] = dat_.to_numpy()\n",
        "\n",
        "\n",
        "  # Return \n",
        "  return X, labels[0].values, list_of_channels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ikIjLu7w4Mm_",
        "colab": {}
      },
      "source": [
        "#Call functions to load all data into variables\n",
        "\n",
        "X_train, labels_train, list_ch_train = read_data_train()\n",
        "X_test, labels_test, list_ch_test = read_data_test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EG3Tq1ajmjOu",
        "colab": {}
      },
      "source": [
        "#Print out the shape of the data to verify its size\n",
        "\n",
        "print('X_train.shape:')\n",
        "print(X_train.shape)\n",
        "print('\\n')\n",
        "\n",
        "print('X_test.shape:')\n",
        "print(X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MwRtb-8ymqDz"
      },
      "source": [
        "\n",
        "### Explaining the shape of the data \n",
        "Dimension 1: There are **7,352** recordings that are labeled as doing one of those activities <br>\n",
        "Dimension 2: In each recording there are **128** time steps <br>\n",
        "Dimension 3: At each timestep, there are sensor **9** values (3 values each for linear acceleration, linear \"body\" acceleration, and angular acceleration) <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPnX-rzBh1Sb",
        "colab_type": "text"
      },
      "source": [
        "### Plot signals from single data point"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDD8nhUuk4FO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Plot signals from a single data point to \"see\" signals data\n",
        "\n",
        "#Get random sample number for plotting\n",
        "sample_num = np.random.randint(0,X_train.shape[0]) #Get a random index\n",
        "example_data_point = X_train[sample_num, :, :]\n",
        "example_label_int = labels_train[sample_num]\n",
        "\n",
        "#Define dictionaries to help label our plot\n",
        "\n",
        "#Note that label integers actually vary from 1-6\n",
        "labels2actions_dict = {1:'WALKING', 2: 'WALKING_UPSTAIRS', 3:'WALKING_DOWNSTAIRS',\n",
        "                   4:'SITTING', 5: 'STANDING', 6: 'LAYING'} \n",
        "\n",
        "signal_labels_dict = {0:'x-accel', 1: 'y-accel', 2:'z-accel',\n",
        "                   3:'x-accel\\n(body)', 4: 'y-accel\\n(body)', 5: 'z-accel\\n(body)',\n",
        "                   6:'x-ang-accel', 7: 'y-ang-accel', 8: 'z-ang-accel'}\n",
        "\n",
        "example_action = labels2actions_dict[example_label_int]\n",
        "\n",
        "#Calculate time vector from sample rate (doesn't vary over dataset)\n",
        "f_s = 50 # constant 50 Hz sampling rate\n",
        "t_vector = np.linspace(start = 0.02, stop = 2.56 , num = 128)\n",
        "\n",
        "#Make subplots for all signal channels\n",
        "fig, axs = plt.subplots(example_data_point.shape[1], 1,\n",
        "                        sharex = True,\n",
        "                        figsize = (20, 15))\n",
        "\n",
        "for signal_ind in range(example_data_point.shape[1]):\n",
        "    axs[signal_ind].plot(t_vector, example_data_point[:, signal_ind])\n",
        "    axs[signal_ind].set_ylabel(signal_labels_dict[signal_ind])\n",
        "\n",
        "# label x-axis\n",
        "axs[-1].set_xlabel('time(s)')\n",
        "\n",
        "# set title based on what activity is being performed\n",
        "plt.suptitle(f'Example {sample_num}, Activity: {example_action}', y = 0.9)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_sTL4YKrsXA6"
      },
      "source": [
        "### Split provided \"training\" set into training and validation sets\n",
        "\n",
        "When we loaded the data in, it comprised two sets, so-called \"training\" and \"test\" sets. Now however, there is a reason to divide the loaded \"training\" set into a new training set and a development set. Together with the test set, this leaves 3 groups of labeled data.\n",
        "\n",
        "Breaking data into these 3 groups: a training set, a development set, and a test set happens when developers are trying to (1) train different models, (2) select the best model to fit the data, and (3) gauging chosen model performance on unlabeled data.\n",
        "\n",
        "The typical sequence might go like this: first, prospective algorithms train on the training set. Afterwards, we can compare the performance of differnt algorithms by evaluating their accuracy on the dev set. Finally, after we select a algorithm based on train/dev set performance, we can get an estimate of real-world performance by computing model performance on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "buuR3HH9yTOw",
        "colab": {}
      },
      "source": [
        "#Split training data up so we can use part of it for a development set\n",
        "\n",
        "#Below is a block to scale training and test data\n",
        "#this would make sure that the magnitudes of all measurements are similar\n",
        "#before we \n",
        "\n",
        "## Uncomment this block to scale time series data\n",
        "#(Performing this scaling will mean that the units of each datapoint are no\n",
        "#longer in standard units)\n",
        "\n",
        "'''\n",
        "scaler = StandardScaler()\n",
        "reshaped_X_train = X_train.reshape((X_train.shape[0]*X_train.shape[1], X_train.shape[2])).copy()\n",
        "reshaped_X_test = X_test.reshape((X_test.shape[0]*X_test.shape[1], X_train.shape[2])).copy()\n",
        "\n",
        "reshaped_X_train = scaler.fit_transform(reshaped_X_train)\n",
        "reshaped_X_test = scaler.transform(reshaped_X_test)\n",
        "\n",
        "X_train = reshaped_X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2]))\n",
        "X_test = reshaped_X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2]))\n",
        "'''\n",
        "\n",
        "#Note: In the split, we stratify based on training labels to ensure that within\n",
        "#each label data is divided proportionately between the new training and\n",
        "#validation sets\n",
        "\n",
        "X_tr, X_vld, lab_tr, lab_vld = train_test_split(X_train, labels_train, \n",
        "                                                stratify = labels_train, random_state = 123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0LiLobJZnscs"
      },
      "source": [
        "### Load one-hot functions (to convert numeric labels)\n",
        "<br> Use these functions to convert all labels into one-hot encoding. This is a common step in machine learning classification problems.\n",
        "\n",
        "When we convert labels to one-hot encoding, each label is represented by a row with a single value of 1 (at the column index of the label) and all other zeros. \n",
        "\n",
        "For example, the encoding: [0,0,0,0,0,1] would represent the last label, LAYING.\n",
        "This label representation is in contrast to previous label encoding where LAYING would just be represented as the integer 5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wrog_9co1HhG",
        "colab": {}
      },
      "source": [
        "#Define function to convert integer labels into one-hot encoding\n",
        "\n",
        "def one_hot(labels, n_class = 6):\n",
        "\t\"\"\" \n",
        "\t\tReplace integer entries in labels with their onehot equivalents\n",
        "\n",
        "\t\tparameters:\n",
        "\n",
        "\t\t\t labels  -- a 1D np.array of integer labels\n",
        "\n",
        "\t\t\t n_class -- the total number of classes\n",
        "\n",
        "\t\treturns:\n",
        "\t\t   y -- 2D numpy array where ith row corresponds to one-hot encoding\n",
        "\t\t\t \t\t\tof the integer in labels[i]\n",
        "\t\"\"\"\n",
        "\t\n",
        "\t# Make an identity matrix (ones on the diagonal, zeros everywhere else)\n",
        "\texpansion = np.eye(n_class)\n",
        " \n",
        "\t# let the ith entry of y be the (label-1)th column of the identity matrix\n",
        "\ty = expansion[labels-1, :]\n",
        "\n",
        "\treturn y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gZQJY0rYnKeC",
        "colab": {}
      },
      "source": [
        "#Convert labels into one-hot encoding (using above function)\n",
        "\n",
        "y_tr = one_hot(lab_tr)\n",
        "y_vld = one_hot(lab_vld)\n",
        "y_test = one_hot(labels_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7bi9-LMuotEG"
      },
      "source": [
        "### Define your Keras model <br>\n",
        "\n",
        "For a straight-forward example of Keras model creation of CNNs you can refer to this example of a simple CNN used for classifying images in the MNIST database: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py (just be sure to note that that network uses 2D convolution, not 1D. So instead of using \"conv2D\" layers, we will be using \"conv1D\" layers.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_xq2TuaP5xJg",
        "colab": {}
      },
      "source": [
        "#Define neural network structure using Keras\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Conv1D(filters=18, kernel_size=2, strides=1, padding=\"same\",  activation = tf.nn.relu, input_shape=(128, 9)))\n",
        "model.add(keras.layers.GlobalMaxPooling1D())\n",
        "model.add(keras.layers.Dense(6, activation=tf.nn.sigmoid))\n",
        "model.summary()   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "X844AtBlqV1Y"
      },
      "source": [
        "### Compile model\n",
        "\n",
        "Before we can start training our Keras model, we need to compile it. When we compile the model, we also specify the optimizer, loss function, learning rate, and metrics. Here is a high-level explanation of what these terms mean. We will have more discussion of them in future lectures. Also, please feel free to follow included links for more information in the Keras documentation.\n",
        "\n",
        "**Loss Function** - This is the quantity that should be minimized when the network is trained. (It is like the mean squared error for a linear regression.) A neural network can use squared error as a loss function, but there are also other options. In the case of a neural network trying to classify samples into 1 of n categories system a common choice is called cross entropy loss.\n",
        "\n",
        "documentation: https://keras.io/api/losses/\n",
        "\n",
        "**Optimizer** - When a neural network is trained, it changes weights in the network to minimize the loss function. The optimizer governs how the neural network iteratively changes its weights as it minimizes loss. Many optimizers use the derivative of the loss function with respect to all the weights to decide which direction to change network weights.\n",
        "\n",
        "documentation: https://keras.io/api/optimizers/\n",
        "\n",
        "**Metrics** - These are quantities that you would like the model to \"report\" during the training process. You can specify a multiple metrics.\n",
        "\n",
        "documentation: https://keras.io/api/metrics/\n",
        "\n",
        "**Learning Rate** - This scalar quantity determines how much the optimizer changes neural network weights between iterations. If this value is too low, it may delay convergence of the optimization. If this value is too high, the network weight optimization might overshoot useful solutions or not converge.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6KSPl0lMqUUZ",
        "colab": {}
      },
      "source": [
        "#Define learning rate, optimizer, metrics, and loss function for your model as you compile it\n",
        "\n",
        "lr = 0.0008 # choose a learning rate\n",
        "model.compile( optimizer=tf.compat.v1.train.AdamOptimizer(lr), \n",
        "              loss=keras.losses.categorical_crossentropy, metrics = ['accuracy'] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pBOK_ZaCq6XR"
      },
      "source": [
        "### Fit your model\n",
        "\n",
        "When you run this cell, the optimizer will attempt to tune the network weights to minimize the loss function. It does this by iteratively calculating the current loss function value of the network and intelligently changing the network weights to reduce that loss (this is where gradient descent comes in). Note that Keras lets us include the validation data as an input to the fitting function so validation accuracy can be reported \n",
        "\n",
        "For reference here are the definitions of a \"batch size\" and an \"epochs\" from the perspective of training a neural network:\n",
        "\n",
        "**Batch Size** - In each iteration of the optimizer, the batch size controls how many datapoints are taken into account when calculating derivatives of the loss function. (If batch size is less than number of samples, there will be multiple optimization iterations per epoch.) More batches take longer, but may reduce overfitting by considering a different set of samples in each optimizer iteration.\n",
        "\n",
        "**Epochs** - This value controls how many times the data be passed through before optimization is finished. Specifying a higher number of epochs will obviously increase training time, but might permit the optimizer to find a better set of network weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3jg5n1PgvGbh",
        "colab": {}
      },
      "source": [
        "#Fit model\n",
        "\n",
        "history = model.fit(X_tr, # Train examples\n",
        "                    y_tr, # Train labels\n",
        "                    epochs=100, # number of epochs (passes through data during training)\n",
        "                    batch_size=600, # number of points to consider in each optimizer iteration\n",
        "                    validation_data=(X_vld, y_vld), #data to use for validation\n",
        "                    verbose=1) #will print information about optimization process"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2YXazeH8udoG"
      },
      "source": [
        "### Evaluate model on test set\n",
        "(After you print your accuracy and loss, play around with the parameters to try and improve the model performance.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k0e7vQOO5w90",
        "colab": {}
      },
      "source": [
        "#Evaluate model on test set\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDAGTMr9xx9p",
        "colab_type": "text"
      },
      "source": [
        "### Conclusions\n",
        "\n",
        "This notebook is meant to give you an example of building a convolutional neural network in Keras. Using convolutional neural networks in Keras is one approach that can be used to solve the week 2 challenge exercise.\n",
        "\n",
        "While building and training your network might only take a handful of lines of code, understanding all the magic going on behind the scenes can and should take time. If you don't find you have a strong intuition for what is happening in a neural network during week 2, dont worry. We will continue explaining the components of neural networks well into week 3 as well."
      ]
    }
  ]
}