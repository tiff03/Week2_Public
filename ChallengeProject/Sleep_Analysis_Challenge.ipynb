{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sleep_Analysis_Challenge.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB6YunD0Gdgc",
        "colab_type": "text"
      },
      "source": [
        "# \"You Snooze, You Win\" Challenge\n",
        "\n",
        "Every year, the [PhysioNet/CinC (Computing in Cardiology) Challenge](https://www.physionet.org/challenge/) invites \"participants to tackle clinically interesting problems that are either unsolved or not well-solved.\" For this year's week 2 machine learning challenge, BWSI has revived a past PhysioNet challenge based on sleep classification.\n",
        "\n",
        "This year's challenge focuses on the classification of nonarousal and arousal timeframes. If you would like to understand the biological implications of the challenge, we recommend reading PhysioNet's [introduction](https://physionet.org/challenge/2018/) of the challenge.\n",
        "\n",
        "For this challenge, you will classify samples into 5 classes (Arousal, NREM1, NREM2, NREM3, REM). Each sample consists of seven physiological signals (O2-M1, E1-M2, Chin1-Chin2, ABD, CHEST, AIRFLOW, ECG) measured at 200 Hz over a 60 second period (12000 timepoints). In this notebook, we provide code to import the data, visualize sample signals, implement an example classifier, and 'score' your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ctSzKHoGr1q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Import libraries ###\n",
        "\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "\n",
        "#set default plotting fonts\n",
        "font = {'family' : 'sans-serif',\n",
        "        'weight' : 'normal',\n",
        "        'size'   : 20}\n",
        "\n",
        "matplotlib.rc('font', **font)\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn import metrics\n",
        "from sklearn.utils import shuffle\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XxIE6ZwGvjh",
        "colab_type": "text"
      },
      "source": [
        "## Loading the Dataset\n",
        "\n",
        "This dataset is a modified version of the PhysioNet/CinC Challenge data, which were contributed by the Massachusetts General Hospitalâ€™s Computational Clinical Neurophysiology Laboratory, and the Clinical Data Animation Laboratory.\n",
        "***\n",
        "**Class labels:**\n",
        "- 0 = Arousal\n",
        "- 1 = NREM1\n",
        "- 2 = NREM2\n",
        "- 3 = NREM3\n",
        "- 4 = REM\n",
        "***\n",
        "**Class descriptions:**\n",
        "\n",
        "<img src=\"https://github.com/BeaverWorksMedlytics2020/Data_Public/blob/master/Images/Week2/sleepStagesTable.svg?raw=true\">\n",
        "\n",
        "***\n",
        "**Physiological signal description:**\n",
        "\n",
        "O2-M1 - posterior brain activity (electroencephalography)\n",
        "\n",
        "E1-M2 - left eye activity (electrooculography)\n",
        "\n",
        "Chin1-Chin2 - chin movement (electromyography)\n",
        "\n",
        "ABD - abdominal movement (electromyography)\n",
        "\n",
        "CHEST - chest movement (electromyography)\n",
        "\n",
        "AIRFLOW - respiratory airflow\n",
        "\n",
        "ECG - cardiac activity (electrocardiography)\n",
        "***\n",
        "Run both cell blocks to get the challenge data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfgbZPNYziXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone repo and move into data directory (only run this once)\n",
        "!git clone https://github.com/BeaverWorksMedlytics2020/Data_Public\n",
        "os.chdir('./Data_Public/ChallengeProjects/Week2/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qANF2BvQG2m3",
        "colab_type": "text"
      },
      "source": [
        "## Loading Data in Memory\n",
        "Run the cell below to extract the raw training and test data. It may take a minute or two to run through. Here are the variables containing the data you will get:\n",
        "\n",
        "* **data_train**: np array shape (4000, 12000, 7). Contains 4000 samples (60s each) of 12000 data points (200Hz x 60s), for 7 different signals. \n",
        "* **labels_train**: np array shape (4000,). Contains ground truth labels for data_train. The order of the labels corresponds to the order of the training data.\n",
        "* **ID_train**: list of 4000 unique IDs. The order of the IDs corresponds to the order of the training data. \n",
        "* **data_test**: np array shape (1000, 12000, 7). Contains 1000 samples (60s each) of 12000 data points (200Hz x 60s), for 7 different signals.\n",
        "* **ID_test**: list of 1000 unique IDs. The order of the IDs corresponds to the order of the training data.\n",
        "\n",
        "We encourage you to print each of these variables to see what they look like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Rw8inOvG5QP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Run once to import data ###\n",
        "\n",
        "def get_file_locs():\n",
        "    '''\n",
        "    find all files in 'training' and 'test' directories and put their names \n",
        "    under 'training' and 'test' keys in the file_dict dictionary\n",
        "    '''\n",
        "\n",
        "    file_dict = {'training':[], 'test':[]}\n",
        "    for data_type in file_dict:\n",
        "        for file in os.listdir('./' + data_type):\n",
        "            file_dict[data_type].append(data_type + '/' + file)\n",
        "    \n",
        "    return file_dict\n",
        "\n",
        "def get_sample_data(data_type, id_number):\n",
        "    '''\n",
        "    get signal data, label, and filename associated with given data type and index num\n",
        "\n",
        "    parameters:\n",
        "\n",
        "     data_type -- Dictates whether sample comes from training set or test set.\n",
        "                 This input must be either 'training' or 'test' (defaults to 'training')\n",
        "\n",
        "     id_number -- Which sample ID should be returned? Must be 0-3999 if data_type is 'training'\n",
        "                 or 0-999 if data_type is 'test' (defaults to random integer from 0-999)\n",
        "  \n",
        "    returns:\n",
        "\n",
        "     sample_data -- dataframe with 1 row and 2 columns-- column \"Signal\" contains a series object \n",
        "                    and column \"Label\" contains numeric label for that sample\n",
        "    '''\n",
        "    file = './' + data_type + '/' + str(id_number) + '.xz'\n",
        "\n",
        "    #sample_data is a dataframe with 1 row and 2 columns--\n",
        "    #\"Signal\" (contains a series object) and \"Label\" (contains numeric label)\n",
        "    sample_data = pd.read_pickle('./' + file)\n",
        "\n",
        "    return sample_data, file.split('/')[2]\n",
        "\n",
        "file_dict = get_file_locs()\n",
        "print(f\"{len(file_dict['training'])} training samples found, {len(file_dict['test'])} test samples found\")\n",
        "\n",
        "data_train = np.zeros((4000, 12000, 7))\n",
        "labels_train = np.zeros(4000)\n",
        "ID_train = []\n",
        "for i in range(4000):\n",
        "  sample_data, ID = get_sample_data('training', i)\n",
        "  data_train[i] = np.array(list(sample_data['Signal']), dtype=np.float).reshape(12000, 7)\n",
        "  labels_train[i] = np.array(list(sample_data['Label']), dtype=np.float)\n",
        "  ID_train.append(ID)\n",
        "  if(i%500==0):\n",
        "    print('Loading training sample ' + str(i))\n",
        "  \n",
        "data_test = np.zeros((1000, 12000, 7))\n",
        "ID_test = []\n",
        "for i in range(1000):\n",
        "  sample_data, ID = get_sample_data('test', i)\n",
        "  data_test[i] = np.array(list(sample_data['Signal']), dtype=np.float).reshape(12000, 7)\n",
        "  ID_test.append(ID)\n",
        "  if(i%500==0):\n",
        "    print('Loading test sample ' + str(i))\n",
        "\n",
        "#keep original copy of data_train, labels_train, ID_train, data_test, and ID_test\n",
        "# (for plotting or reference in case variables are shuffled later on)\n",
        "data_train_orig = data_train.copy()\n",
        "labels_train_orig = labels_train.copy()\n",
        "ID_train_orig = ID_train.copy()\n",
        "data_test_orig = data_test.copy()\n",
        "ID_test_orig = ID_test.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFZ0IhXKgJSI",
        "colab_type": "text"
      },
      "source": [
        "## Data Visualization\n",
        "\n",
        "Run the cell below to visualize the raw data for a single 60s sample for all 7 signals. We strongly urge you to visualize a few different samples to get a feel for how the data looks and how much each signal can vary in amplitude and frequency from sample to sample. You can change which sample is being visualized by changing the arguments to the graph_signals call."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6x9vTz5HE1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Define some useful functions to retrieve and plot data from individual samples ###\n",
        "\n",
        "\"\"\" Initalize key reference dictionaries \"\"\"\n",
        "sig_dict = {0:'O2-M1', 1:'E1-M2', 2:'Chin1-Chin2', 3:'ABD', 4:'CHEST', 5:'AIRFLOW', 6:'ECG'}\n",
        "sig_type_dict = {0:'Time (s)', 1:'Frequency (Hz)'}\n",
        "stage_dict = {0:'Arousal', 1:'NREM1', 2:'NREM2', 3:'NREM3', 4:'REM'}\n",
        "\n",
        "def graph_signals(data_type = 'training', id_number = None):\n",
        "    '''\n",
        "    Create a graph of multiple signals, with rows representing signal class, and\n",
        "    columns representing signal domain (i.e. time or frequency) \n",
        "    \n",
        "    inputs:\n",
        "    \n",
        "      parameters:\n",
        "\n",
        "      data_type -- Dictates whether sample comes from training set or test set.\n",
        "                 This input must be either 'training' or 'test' (defaults to 'training')\n",
        "\n",
        "      id_number -- Which sample ID should be returned? Must be 0-3999 if data_type is 'training'\n",
        "                 or 0-999 if data_type is 'test' (defaults to random integer from 0-999)\n",
        "  \n",
        "    '''\n",
        "\n",
        "    assert data_type in ['test','training'], 'data_type should be either \"test\" or \"training\"'\n",
        "\n",
        "    if id_number is None:\n",
        "      if data_type == 'training':\n",
        "        id_number = random.randint(0,3999)\n",
        "      else:\n",
        "        id_number = random.randint(0,999)\n",
        "\n",
        "    #set time_signal_y based on 'test' or 'training' input\n",
        "    if data_type == 'training':\n",
        "        time_signal_y = data_train_orig[id_number,:,:]\n",
        "    else:\n",
        "        time_signal_y = data_test_orig[id_number,:,:]\n",
        "\n",
        "    #Get signal values in time domain from either data_train_orig, or data_test_orig\n",
        "    time_signal_x = np.arange(0, 60, step = 1/200) #this time axis doesn't change\n",
        "\n",
        "    #Use time-domain signals to get frequency domain signals\n",
        "    num_datapoints = time_signal_x.shape[0]\n",
        "    freq_signal_x = np.arange(num_datapoints//2 + 1)/60\n",
        "    freq_signal_y = np.ndarray(shape = (num_datapoints//2 + 1, 7))\n",
        "    freq_signal_y = np.abs(np.fft.rfft(time_signal_y, axis = 0))\n",
        "\n",
        "    plt.figure(figsize = (20, 18))\n",
        "    title_list = ['time domain', 'freq domain']\n",
        "    for signal_index in range(7): #7 classes of signal (O2-M1, E2-M2, etc.)\n",
        "        for signal_domain_index in range(2): #2 types of signal (time and freq domain)\n",
        "            plt.subplot(7, 2, 2*signal_index + signal_domain_index + 1)\n",
        "            if signal_index==0:\n",
        "                plt.title(title_list[signal_domain_index])\n",
        "            if signal_domain_index == 0: #plot time domain signal\n",
        "                plt.plot(time_signal_x, time_signal_y[:,signal_index])\n",
        "            if signal_domain_index == 1: #plot freq domain signal\n",
        "                plt.plot(freq_signal_x, freq_signal_y[:,signal_index])\n",
        "            plt.ylabel(sig_dict[signal_index])\n",
        "        plt.xlabel(sig_type_dict[signal_domain_index])\n",
        "    plt.show()\n",
        "\n",
        "# We can now use the above functions to retrieve data for a single sample \n",
        "# (in the test or training set) and then plot it\n",
        "\n",
        "#change these 2 lines to plot different samples (note that test set only has 1000 samples)\n",
        "data_type = 'training'\n",
        "id_number = random.randint(0,3999)\n",
        "\n",
        "#print signal ID and the sample label (if known)\n",
        "if data_type == 'training': \n",
        "    print(data_type.title(), str(id_number), '(' + stage_dict[id_number//800] + ')')\n",
        "else: \n",
        "    print(data_type.title(), str(id_number), '(Unknown)')\n",
        "\n",
        "#plot data from this sample\n",
        "graph_signals(data_type = data_type, id_number = id_number)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXsZa50WHGZR",
        "colab_type": "text"
      },
      "source": [
        "## Example Classifier\n",
        "\n",
        "Below is an example of a mediocre classifier for this dataset. It is a simple neural network that uses the spectral flatness of all signal classes its input features--that is, it has 7 input features.\n",
        "\n",
        "While the example classifier makes use of a neural network, we encourage you to utilize any ML algorithm that you feel would be appropriate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUhVo7jkHIdS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Define functions that calculate the features of a single sample ###\n",
        "\n",
        "import scipy.stats.mstats as mstats\n",
        "\n",
        "#I'm going to use spectral_flatness of the fft as a feature that could be used\n",
        "\n",
        "def spectral_flatness(fft, zero_thresh = None): \n",
        "  N = len(fft) \n",
        "  magnitude = abs(fft[:N//2])* 2 / N \n",
        "\n",
        "  if(zero_thresh is not None):\n",
        "      magnitude[magnitude < zero_thresh] = zero_thresh\n",
        "\n",
        "  if(any(magnitude<zero_thresh)):\n",
        "    raise ValueError(\"THis shouldn't happen\")\n",
        "\n",
        "  sf = mstats.gmean(magnitude) / np.mean(magnitude) \n",
        "  \n",
        "  return sf\n",
        "\n",
        "def get_features_for_single_sample(data_type = 'training', id_number = None):\n",
        "    '''\n",
        "    Calculate some number feature values for a single sample\n",
        "\n",
        "    parameters:\n",
        "\n",
        "      data_type -- Dictates whether sample comes from training set or test set.\n",
        "                 This input must be either 'training' or 'test' (defaults to 'training')\n",
        "\n",
        "      id_number -- Which sample ID should be returned? Must be 0-3999 if data_type is 'training'\n",
        "                 or 0-999 if data_type is 'test' (defaults to random integer from 0-999)\n",
        "  \n",
        "    returns:\n",
        "\n",
        "      feature_vector -- 1d array containing all feature values for this sample\n",
        "    '''\n",
        "\n",
        "    assert data_type in ['test','training'], 'data_type should be either \"test\" or \"training\"'\n",
        "\n",
        "    if id_number is None:\n",
        "        id_number=random.randint(0,999)\n",
        "\n",
        "    if data_type == 'training':\n",
        "        time_signal_y = data_train_orig[id_number,:,:]\n",
        "    else:\n",
        "        time_signal_y = data_test_orig[id_number,:,:]\n",
        "  \n",
        "    #---Populate feature vector ---\n",
        "    #(Example uses spectral flatness, but you can populate feature_vector with \n",
        "    # anything that seems useful)\n",
        "\n",
        "    #get an fft for all signals in this sample\n",
        "    fft_arr = np.fft.rfft(time_signal_y, axis = 0)\n",
        "    \n",
        "    #Initialize feature vector to zeros of right size\n",
        "    feature_vector = np.zeros((time_signal_y.shape[1],1))\n",
        "\n",
        "    #Enumerate over all classes of signal, calculate spectral flatness for each and append value to feature_vector\n",
        "    for ind in range(fft_arr.shape[1]): #for every class of signal\n",
        "        feature_vector[ind] = spectral_flatness(fft_arr[:,ind], zero_thresh = 10**-10) #calculate spectral flatness and add it to a feature vector \n",
        "\n",
        "    #---Finished populating feature vector---\n",
        "    #(now feature_vector is populated with hand-engineered features for this sample)\n",
        "\n",
        "    return feature_vector\n",
        "    \n",
        "def get_features(data_type, num_samples):\n",
        "    '''\n",
        "    Retrieve features from num_samples samples in either test or training set, \n",
        "    return features for each sample and index of each sample\n",
        "    \n",
        "    inputs:\n",
        "    \n",
        "      data_type -- Dictates whether sample comes from training set or test set.\n",
        "                 This input must be either 'training' or 'test' (defaults to 'training')\n",
        "\n",
        "      num_samples -- For how many samples should features be calculated?\n",
        "                  (should not exceed total samples in a given data_type)\n",
        "  \n",
        "    returns:\n",
        "\n",
        "      features -- n x p array of feature values for all samples where rows \n",
        "                  correspond to samples and columns correspond to feature number\n",
        "\n",
        "      order -- 1d array where index i contains the index of row i of the feature vector\n",
        "               (only needed if shuffling takes place later)\n",
        "    '''\n",
        "\n",
        "    features, order = np.array([]), np.array([])\n",
        "    for i in range(num_samples):\n",
        "        single_sample_features = get_features_for_single_sample(data_type = data_type, id_number = i)\n",
        "        features = np.hstack((features, single_sample_features)) if features.size else single_sample_features\n",
        "        order = np.append(order, i).astype(np.float32)\n",
        "\n",
        "    #ensure features are of type float32    \n",
        "    features = features.astype(np.float32)\n",
        "\n",
        "    return np.transpose(features), order\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSB4-mn-Chct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Run every time you modify your feature extraction ###\n",
        "\n",
        "print('calculating features for training data...')\n",
        "train_data, train_order = get_features('training', 4000)\n",
        "\n",
        "\n",
        "print('calculating features for test data...')\n",
        "test_data, test_order = get_features('test', 1000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVNgpgciw6p4",
        "colab_type": "text"
      },
      "source": [
        "## Visualize Entire Dataset's Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN0BBD3fHKRq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Create label array for all training samples using categorical datatype ###\n",
        "\n",
        "train_labels = np.ndarray(shape = (1, 4000))\n",
        "\n",
        "#set labels to integers first\n",
        "for i in range(4000):\n",
        "    train_labels[0][i] = i//800 # This is a way to label each entry (since classes are in order)\n",
        "\n",
        "#convert labels to onehot, ensure type is float32\n",
        "train_labels = tf.keras.utils.to_categorical(train_labels[0], 5)\n",
        "train_labels = train_labels.astype(np.float32)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fTxkQFfNKgJD",
        "colab": {}
      },
      "source": [
        "### Run whenever you want to check or view your data and labels ###\n",
        "print(f\"train_data.shape = {train_data.shape} \\n train_labels.shape = {train_labels.shape}\\n\\n\")\n",
        "print(f\"train data:\\n {train_data}\\n\\ntrain labels:\\n {train_labels}\")\n",
        "#plot feature values and label values to show data clearly\n",
        "fig, (ax1, ax2) = plt.subplots(ncols = 2, figsize= (30,15))\n",
        "\n",
        "#Note: if your features are not scaled to the range 0->1 you might want to change vmin and vmax\n",
        "featureImg = ax1.imshow(train_data[:,:], aspect = 'auto', interpolation = 'None')# vmin = 0, vmax = 1.0)\n",
        "ax1.set_title('Feature Values')\n",
        "ax1.set_ylabel('Sample Number')\n",
        "ax1.set_xlabel('Feature Number')\n",
        "fig.colorbar(featureImg, ax = ax1, boundaries = np.linspace(0.0, 1.0, 50), ticks = [0,0.5,1])\n",
        "\n",
        "from matplotlib.colors import ListedColormap\n",
        "bw_cmap = ListedColormap(np.array([[0,0,0,1],[1,1,1,1]])) # make a colormap manually\n",
        "labelImg = ax2.imshow(train_labels, aspect = 'auto', cmap = bw_cmap,interpolation = 'None', vmin = 0, vmax = 1.0)\n",
        "ax2.set_title('Label Values')\n",
        "ax2.set_xlabel('Label')\n",
        "ax2.set_xticks(range(len(stage_dict)))\n",
        "ax2.set_xticklabels([stage_dict[i] for i in range(len(stage_dict))])\n",
        "fig.colorbar(labelImg, ax = ax2, boundaries = np.linspace(-0.5, 1.5,3), ticks = [0, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_0M_8mJHORZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Run every time you modify your feature extraction ###\n",
        "#\n",
        "# This cell splits up the labeled data into 3 subsets (training data, validation\n",
        "# data, and test data)\n",
        "#\n",
        "# This 3-way split is very common in training a supervised learning model. The purpose of\n",
        "# each partition is as follows:\n",
        "#\n",
        "# 1) Training data is used to adjust model weights (so-called model parameters)\n",
        "# 2) Validation data is used to gauge how well model generalizes (allowing user \n",
        "#    to compare between different trained models and/or non-trained model \n",
        "#    parameters (hyperparameters))\n",
        "# 3) Test data is used to provide an indicator of real-world performance (it \n",
        "#    should not be used when deciding between models because this would result \n",
        "#    in overfitting!)\n",
        "\n",
        "train_data_shuffled, train_labels_shuffled = shuffle(train_data, train_labels, random_state = 25)\n",
        "\n",
        "\"\"\" val_size (int) must be from 0-4000 \"\"\"\n",
        "val_size = 1000\n",
        "mocktest_size = 500\n",
        "\n",
        "val_data = train_data_shuffled[:val_size]\n",
        "mocktest_data = train_data_shuffled[val_size:val_size + mocktest_size]\n",
        "partial_train_data = train_data_shuffled[val_size + mocktest_size:]\n",
        "\n",
        "val_labels = train_labels_shuffled[:val_size]\n",
        "mocktest_labels = train_labels_shuffled[val_size:val_size + mocktest_size]\n",
        "partial_train_labels = train_labels_shuffled[val_size + mocktest_size:]\n",
        "\n",
        "training_set = tf.data.Dataset.from_tensor_slices((partial_train_data, partial_train_labels))\n",
        "training_set = training_set.batch(40) #set batch size\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOcj_uuUHP3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Run every time you change set of parameters ###\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "\"\"\" Modify to your heart's and algorithm's content ^_^ \"\"\"\n",
        "\n",
        "model.add(tf.keras.layers.Dense(64, activation=tf.nn.relu, input_shape=(train_data.shape[1],)))\n",
        "model.add(tf.keras.layers.Dense(64, activation=tf.nn.relu))\n",
        "# we should end with a softmax to ensure outputs behave like probabilities\n",
        "#(i.e. sum to 1)\n",
        "model.add(tf.keras.layers.Dense(5, activation=tf.nn.softmax)) \n",
        "\n",
        "opt = tf.keras.optimizers.RMSprop(learning_rate=0.00005)\n",
        "#Another potential optimizer\n",
        "#opt = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhhsAXXJHRvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Run whenever you want to train and validate your model ###\n",
        "\n",
        "\"\"\"\n",
        "EPOCHS (int) the number of times the optimization algorithm passes\n",
        "through the full dataset (calculating errors and derivatives) to update weights\n",
        "(One pass through the data is called an \"epoch\")\n",
        "\"\"\"\n",
        "\n",
        "EPOCHS = 50\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    for signals, labels in training_set:\n",
        "        tr_loss, tr_accuracy = model.train_on_batch(signals, labels)\n",
        "    val_loss, val_accuracy = model.evaluate(val_data, val_labels)\n",
        "    print(('Epoch #%d\\t Training Loss: %.2f\\tTraining Accuracy: %.2f\\t'\n",
        "         'Validation Loss: %.2f\\tValidation Accuracy: %.2f')\n",
        "         % (epoch + 1, tr_loss, tr_accuracy,\n",
        "         val_loss, val_accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRk6Xh_LHTXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Run once after you have finished training your model ###\n",
        "\n",
        "test_pred = model.predict(test_data)\n",
        "test_output = np.ndarray(shape = (1000, 6))\n",
        "\n",
        "# \"\"\" Add column with file number \"\"\"\n",
        "for i in range(1000):\n",
        "    test_output[i] = np.append(test_pred[i], test_order[i]) \n",
        "test_dataframe = pd.DataFrame(test_output)\n",
        "\n",
        "\"\"\" Sort dataframe according to file number \"\"\"\n",
        "sorted_test_dataframe = test_dataframe.sort_values(by=[5])\n",
        "\n",
        "\"\"\" Drop file number column \"\"\"\n",
        "processed_test_dataframe = sorted_test_dataframe.drop(sorted_test_dataframe.columns[5], axis=1)\n",
        "\n",
        "print(test_dataframe.head(), '\\n\\n', sorted_test_dataframe.head(), '\\n\\n', processed_test_dataframe.head())\n",
        "\n",
        "file = 'unladenSwallow.xz'\n",
        "processed_test_dataframe.to_pickle(file)\n",
        "test_dataframe = pd.DataFrame(test_output)\n",
        "os.listdir('.')\n",
        "files.download(file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bduvOuTtHVfK",
        "colab_type": "text"
      },
      "source": [
        "## Submitting Your Model\n",
        "\n",
        "After training your classifier, run it on the test data to generate your predictions. Each class for a test sample should have an associated probability (between 0 and 1). Below are the parameters for the prediction format and export:\n",
        "\n",
        "- Your predictions should be in a pandas dataframe with 5 columns (classes) and 1000 rows (samples). Note that your predictions must follow the original test sample order (0.xz, 1.xz, 2.xz, ...). You only need to worry about this if you shuffled the test samples or stored the samples in an unordered data structure (dictionaries and sets). If this is the case, you should 1) add a separate column in your pandas dataframe with the file number for each sample; 2) sort the dataframe using this column; and 3) drop the column. These steps have been noted in the code below.\n",
        "- The predictions dataframe should be exported as an .xz file using dataframe.to_pickle() followed by files.download().\n",
        "\n",
        "Example code of the prediction format and export is presented in the cell block below. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TiEUxHcZz_D",
        "colab_type": "text"
      },
      "source": [
        "Your model will be evaluated on Area Under the ROC Curve (ROCAUC), Matthews Correlation Coefficient (MCC) and creativity. There will be a \"winning\" group for each of these categories.\n",
        "\n",
        "If you are finished early, consider trying other ML algorithms and/or implementing multiple feature extraction methods. You can also help other groups if you finish early."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDQWbhUK6cMt",
        "colab_type": "text"
      },
      "source": [
        "## How Your Model Will Be Evaluated\n",
        "\n",
        "- **Area Under the ROC Curve (AUCROC)**: The receiver operating characteristic (ROC) curve plots the true positive rate (sensitivity/recall) against the false positive rate (fall-out) at many decision threshold settings. The area under the curve (AUC) measures discrimination, the classifier's ability to correctly identify samples from the \"positive\" and \"negative\" cases. Intuitively, AUC is the probability that a randomly chosen \"positive\" sample will be labeled as \"more positive\" than a randomly chosen \"negative\" sample. In the case of a multi-class ROC curve, each class is considered separately before taking the weighted average of all the class results. Simply put, the class under consideration is labeled as \"positive\" while all other classes are labeled as \"negative.\" Below is the multi-class ROC curve for the example classifier. The AUCROC score should be between 0 and 1, in which 0.5 is random classification and 1 is perfect classification.\n",
        "\n",
        "<img src=\"https://github.com/BeaverWorksMedlytics2020/Data_Public/blob/master/Images/Week2/MultiClassRocCurve_exampleClassifier.png?raw=true\" width=\"600\" height=\"500\">\n",
        "\n",
        "- **Matthews Correlation Coefficient (MCC)**: The MCC measures the quality of binary classifications, irrespective of the class sizes. Importantly, it is typically regarded as a balanced measure since it considers all values in the 2x2 contingency table (TP, FP, TN, FN). For this challenge, the binary classes will be \"Arousal\" (Arousal) and \"Nonarousal\" (NREM1, NREM2, NREM3, REM). The MCC score should be between -1 and 1, in which 0 is random classification and 1 is perfect classification.\n",
        "\n",
        " ![alt text](https://wikimedia.org/api/rest_v1/media/math/render/svg/5caa90fc15105b74b59a30bbc9cc2e5bd43a13b7)\n",
        "\n",
        "Using these metrics, the example classifier has the following scores on test data:\n",
        "- AUCROC: 0.727\n",
        "- MCC: 0.163\n",
        "- Creativity: ( Í¡Â° ÍœÊ– Í¡Â°)\n",
        "\n",
        "Below is the code used to calculate the AUCROC and MCC metrics when evaluating your classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOpioHig8688",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_pred = pd.DataFrame(model.predict(mocktest_data))\n",
        "test_predict = test_pred.idxmax(axis=1)\n",
        "test_labels = [ np.where(label==1)[0][0] for label in mocktest_labels]\n",
        "test_labels_one_hot = pd.DataFrame(mocktest_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIlfwfksLp-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fpr = {}\n",
        "tpr = {}\n",
        "roc_auc = {}\n",
        "\n",
        "plt.figure(figsize=(14,10))\n",
        "for i in range(5):\n",
        "    fpr[i], tpr[i], _ = metrics.roc_curve(test_labels_one_hot.iloc[:, i], test_pred.iloc[:, i])\n",
        "    roc_auc[i] = metrics.auc(fpr[i], tpr[i])\n",
        "    plt.plot(fpr[i], tpr[i], label = stage_dict[i] + ', ' + str(i))\n",
        "\n",
        "plt.plot([0, 1], [0, 1])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Multi-Class ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "fpr[\"micro\"], tpr[\"micro\"], _ = metrics.roc_curve(test_labels_one_hot.values.ravel(), test_pred.values.ravel())\n",
        "roc_auc_agg = metrics.auc(fpr[\"micro\"], tpr[\"micro\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO1I5lme8oya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_true = []\n",
        "y_pred = []\n",
        "for i in range(test_pred.shape[0]):\n",
        "    if test_predict.iloc[i]==0: y_pred.append(1)\n",
        "    else: y_pred.append(-1)\n",
        "    if test_labels[i]==0: y_true.append(1)\n",
        "    else: y_true.append(-1)\n",
        "mcc = metrics.matthews_corrcoef(y_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fo7edcTTC6yD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(roc_auc_agg, mcc)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}